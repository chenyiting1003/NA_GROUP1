---
downloads:
  - url: 'https://raw.githubusercontent.com/chenyiting1003/NA_GROUP1/main/reference_0013.bib'
    path: reference_0013.bib
  - url: null
    path: harvard-cite-them-right.csl
bibliography: reference_0013.bib
csl: harvard-cite-them-right.csl
title: NA_Group1's Group Project
execute:
  echo: false
  freeze: true
format:
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto Flex
    monofont: Roboto Mono
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

```{python}
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
from matplotlib.patches import Patch
import requests
import gzip
import io

# 1. Define the URL of the file
url = "https://data.insideairbnb.com/united-kingdom/england/london/2024-09-06/data/listings.csv.gz"

# 2. Download the file
response = requests.get(url)

# 3. Read the compressed file
with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as gz:
    listings = pd.read_csv(gz)
```

```{python}
from tabulate import tabulate

# Ensure the uniqueness of host_id
unique_hosts = listings[['host_id', 'host_total_listings_count', 'room_type']].drop_duplicates()

# Create grouping ranges
bins = [0, 1, 2, 3, 10, 50, 100, 200, float('inf')]
labels = ['1', '2', '3', '4 to 10', '11 to 50', '51 to 100', '101 to 200', '200 or more']
unique_hosts['host_group'] = pd.cut(unique_hosts['host_total_listings_count'], bins=bins, labels=labels)

# Calculate the number and percentage of hosts (based on unique hosts)
grouped = unique_hosts.groupby('host_group', observed=False)
summary = grouped.size().reset_index(name='Number of Hosts')
summary['% of Hosts'] = (summary['Number of Hosts'] / summary['Number of Hosts'].sum() * 100).round(2)

# Calculate the room type percentage (based on unique hosts)
room_type_counts = grouped['room_type'].value_counts(normalize=True).unstack(fill_value=0) * 100
room_type_counts = room_type_counts.round(2)  # Retain two decimal places

# Ensure only 3 columns for room types
room_type_counts = room_type_counts[['Entire home/apt', 'Private room', 'Shared room']]

# Merge the results
result = summary.merge(room_type_counts, left_on='host_group', right_index=True, how='left')

# Rename columns to match the table format
result.columns = ['No. listings linked to host ID', 'Number of Hosts', '% of Hosts',
                  '% Entire home/apt', '% Private room', '% Shared room']
```

```{python}
# Original data (excluding the TOTAL row)
data = {
    "No. listings linked to host ID": ["1", "2", "3", "4 to 10", "11 to 50", "51 to 100", "101 to 200", "200 or more"],
    "Number of Hosts": [29616, 13236, 6177, 8820, 2298, 264, 88, 66],
    "% of Hosts": [48.90, 21.85, 10.20, 14.56, 3.79, 0.44, 0.15, 0.11],
    "% Entire home/apt": [62.34, 56.51, 55.76, 58.13, 68.89, 74.62, 73.86, 69.70],
    "% Private room": [37.14, 42.91, 43.53, 41.00, 29.98, 22.35, 25.00, 24.24],
    "% Shared room": [0.50, 0.57, 0.65, 0.62, 0.91, 0.76, 1.14, 3.03],
}

# Create DataFrame
df = pd.DataFrame(data)

# Calculate the TOTAL row and round to two decimal places
total_row = {
    "No. listings linked to host ID": "TOTAL",
    "Number of Hosts": df["Number of Hosts"].sum(),
    "% of Hosts": round(100.00, 2),
    "% Entire home/apt": round((df["% Entire home/apt"] * df["Number of Hosts"]).sum() / df["Number of Hosts"].sum(), 2),
    "% Private room": round((df["% Private room"] * df["Number of Hosts"]).sum() / df["Number of Hosts"].sum(), 2),
    "% Shared room": round((df["% Shared room"] * df["Number of Hosts"]).sum() / df["Number of Hosts"].sum(), 2),
}

# Add the TOTAL row
df = pd.concat([df, pd.DataFrame([total_row])], ignore_index=True)

# Round all decimal columns to two decimal places
decimal_columns = ["% of Hosts", "% Entire home/apt", "% Private room", "% Shared room"]
df[decimal_columns] = df[decimal_columns].round(2)

# Create the plot
fig, ax = plt.subplots(figsize=(15, 5))  # Set chart size
ax.axis('tight')
ax.axis('off')

# Create the table
table = ax.table(
    cellText=df.values,
    colLabels=df.columns,
    cellLoc='center',
    loc='center'
)

fig.suptitle("Table 1: Number of hosts with one or more active Airbnb listings, \n distribution of host percentages, and listing types managed by hosts",
             fontsize=14, fontweight='bold', y=1)

# Set table styles
table.auto_set_font_size(False)
table.set_fontsize(10)

# Adjust column widths and row heights
row_height = 0.1  # Height for each row
first_col_width = 0.3  # Width of the first column
other_col_width = 0.15  # Width of other columns

# Iterate over each cell to set width, height, and styles
for (row, col), cell in table.get_celld().items():
    cell.set_edgecolor('black')  # Set border color
    cell.set_linewidth(0.8)  # Border width

    # Set header row styles
    if row == 0:  # Header row
        cell.set_facecolor("#D9D9D9")  # Gray background
        cell.set_text_props(fontweight='bold')  # Bold font
        row_height = 0.15
    # Set TOTAL row styles
    elif row == len(df):  # TOTAL row
        cell.set_facecolor("#D9D9D9")  # Gray background
        cell.set_text_props(fontweight='bold')  # Bold font
    # Set second row styles
    elif row == 1:  # Second row
        cell.set_facecolor("#D9EAF7")  # Light blue background

    # Set column widths
    if col == 0:  # First column
        cell.set_width(first_col_width)
    else:
        cell.set_width(other_col_width)
    # Set row height
    cell.set_height(row_height)

# Display the table
plt.show()
```

The data reveals that 48.9% of hosts are non-professional, typically individual homeowners renting out spare rooms or properties part-time. In contrast, professional hosts, who manage multiple listings, control a significant share, highlighting Airbnb's important for commercial property management[@li_pros_2016;@kwok_pricing_2019].
Entire home/apartment listings dominate the market, especially among professional hosts, where their share continues to grow. Meanwhile, shared rooms account for a minimal portion, reflecting the marketâ€™s clear preference for private and independent spaces.

```{python}
# URL of the GitHub file, scrape from the GLA's Housing Research Note 4
url1 = "https://github.com/chenyiting1003/NA_GROUP1/raw/refs/heads/main/airbnb_borough_data_2019.csv"

# Read the CSV file into a DataFrame
airbnb_2019 = pd.read_csv(url1)
```

```{python}
# Group the listings data by neighbourhood_cleansed and room_type
listings_grouped = listings.groupby(['neighbourhood_cleansed', 'room_type']).size().unstack(fill_value=0).reset_index()

# Dynamically handle column names
listings_grouped.columns = ['Borough'] + listings_grouped.columns[1:].tolist()

# Standardize Borough names to lowercase for merging with airbnb_2019
listings_grouped['Borough'] = listings_grouped['Borough'].str.lower()
airbnb_2019['Borough'] = airbnb_2019['Borough'].str.lower()

# Merge the two DataFrames
combined_data = pd.merge(
    airbnb_2019,
    listings_grouped,
    on='Borough',
    how='outer',
    suffixes=('_2019', '_2024')  # Add suffixes to avoid column name conflicts
)

# Fill missing values with 0
combined_data.fillna(0, inplace=True)

# Drop the "Total" and "Hotel room" columns (if they exist)
columns_to_drop = ['Total', 'Hotel room']
cleaned_data = combined_data.drop(columns=[col for col in columns_to_drop if col in combined_data.columns])
```

```{python}
# Replace specific names in the Borough column
cleaned_data['Borough'] = cleaned_data['Borough'].replace({
    'hammersmith and fulham': 'H&F',
    'kensington and chelsea': 'K&C',
    'barking and dagenham': 'B&D'
}, regex=False)
```

```{python}
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

# Define a 7x8 layout of geographical regions
borough_layout = [
    [None, None, None, None, "Enfield", None, None, None],
    [None, None, None, "Harrow", "Barnet", "Haringey", "Waltham Forest", None],
    ["Hillingdon", "Ealing", "Brent", "Camden", "Islington", "Hackney", "Redbridge", "Havering"],
    ["Hounslow", "H&F", "K&C", "Westminster", "City", "Tower Hamlets", "Newham", "B&D"],
    [None, "Kingston", "Wandsworth", "Lambeth", "Southwark", "Lewisham", "Greenwich", "Bexley"],
    [None, None, "Richmond", "Merton", "Croydon", "Bromley", None, None],
    [None, None, None, "Sutton", None, None, None, None],
]

# Set a unified maximum Y-axis range to 12000
y_max = 12000

# Assign colors to room types
color_mapping = {
    'Entire home/apt': "#4472C4",  # Blue
    'Private room': "#EDC586",  # Yellow
    'Shared room': "#E86C74",  # Red
}

# Create a 7x8 canvas with subplots (adjust spacing for compactness)
fig, axes = plt.subplots(nrows=7, ncols=8, figsize=(16, 12), gridspec_kw={'wspace': 0.1, 'hspace': 0.4})

# Iterate through the layout and populate subplots
for i, row in enumerate(borough_layout):
    for j, borough in enumerate(row):
        ax = axes[i, j]
        
        if borough is None:  # Turn off subplot if there is no borough
            ax.axis("off")
            continue
        
        # Set the background color of the subplot to light gray
        ax.set_facecolor("#F2F2F2")
        
        # Remove subplot borders
        for spine in ax.spines.values():
            spine.set_visible(False)
        
        # Extract data for the current region
        subset = cleaned_data[cleaned_data['Borough'].str.contains(borough, case=False)]
        
        # Prepare data for plotting
        years = [2019, 2024]  # Years
        room_types = ['Entire home/apt', 'Private room', 'Shared room']  # Room types
        data = {
            room: [subset[f"{room}_2019"].sum(), subset[f"{room}_2024"].sum()] 
            for room in room_types
        }

        # Create a stacked area chart
        bottom = [0] * len(years)  # Initial base of the stack
        for room_type in room_types:
            ax.fill_between(
                years, bottom, [sum(x) for x in zip(bottom, data[room_type])],
                label=room_type, alpha=0.9, color=color_mapping[room_type]
            )
            bottom = [sum(x) for x in zip(bottom, data[room_type])]  # Update the base
        
        # Add a title with a background box
        title_text = borough.title()
        ax.text(
            0.5, 1.1, title_text,  # Position of the title
            fontsize=10, color="#333333", ha='center', va='center',
            transform=ax.transAxes
        )

        ax.set_ylim(0, y_max)  # Set the unified Y-axis range
        
        # Display logic for X and Y axes
        if borough in ["Enfield", "Harrow", "Hillingdon", "Hounslow", "Kingston", "Richmond", "Sutton"]:
            # Show Y-axis ticks and labels
            ax.set_yticks(range(0, y_max + 1, 3000))
            ax.yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))  # Format ticks
            ax.tick_params(axis='y', labelsize=8)
            ax.set_ylabel("Listings", fontsize=9)
            
            # Show X-axis ticks and labels
            ax.set_xticks(years)
            ax.set_xticklabels(["2019", "2024"], fontsize=9)
        else:
            # Hide Y-axis ticks
            ax.set_yticks(range(0, y_max + 1, 3000))
            ax.tick_params(axis='y', labelsize=0)
            
            # Hide X-axis ticks and labels
            ax.set_xticks(years)
            ax.tick_params(axis='x', labelsize=0)

        # Add gridlines
        ax.grid(visible=True, linestyle='--', linewidth=0.5, alpha=0.6)

# Add a global legend
fig.subplots_adjust(bottom=0)  # Increase bottom space for the legend
fig.legend(
    color_mapping.keys(), loc="lower right", ncol=3, fontsize=12, 
    title="Room Type", title_fontsize=13, frameon=True
)
fig.suptitle("Figure 1: Number of Airbnb Listings in London Boroughs (2019 vs 2024)", fontsize=18, fontweight='bold')
plt.show()
```

In figure1, professional hostsâ€™ listings are concentrated in central London and tourist areas(Westminster, Kensington). In contrast, non-professional hosts(aim to supplement income), tend to have their listings on the outskirts of London.
The Airbnb market in London has the polarization between the sharing-economy and commercialization.

## 7.Drawing on your previous answers, and supporting your response with evidence, how could the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London?

```{python}
# Standard library imports
import os
import io 
import gzip
import requests as rq  # Used for downloading data
from io import BytesIO  # For handling in-memory file-like objects

# Third-party imports
import pandas as pd
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Statsmodels imports
import statsmodels.api as sm
from statsmodels.tools.tools import add_constant
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.diagnostic import het_breuschpagan
from statsmodels.stats.stattools import durbin_watson

# Spatial analysis imports
from libpysal.weights import Queen
from esda.moran import Moran

# External dependency installation
!pip install kagglehub
import kagglehub
```

```{python}
# Load the dataset for Inside Airbnb updated September 2024
url = "https://data.insideairbnb.com/united-kingdom/england/london/2024-09-06/data/listings.csv.gz"

# geojson dataset
url2 = "https://data.insideairbnb.com/united-kingdom/england/london/2024-09-06/visualisations/neighbourhoods.geojson"

url3 = "https://github.com/chenyiting1003/NA_GROUP1/raw/refs/heads/main/datadownload_local_rent.xlsx"
```

```{python}
# Load the dataset for Inside Airbnb updated September 2024
listings_2024 = pd.read_csv(BytesIO(rq.get(url).content), compression='gzip')

# Filter rows where 'price' is not 0 and room_type is "Entire home/apt"
filtered_listings_2024 = listings_2024[
    (listings_2024["price"] != 0) & (listings_2024["room_type"] == "Entire home/apt")
]
```

```{python}
# geojson dataset
neighborhood_gdf = gpd.read_file(url2)

# Convert CRS to British National Grid (EPSG:27700) and calculate area in square kilometers
neighborhood_gdf = neighborhood_gdf.to_crs(epsg=27700)
neighborhood_gdf['area_km2'] = neighborhood_gdf.geometry.area / 1e6
```

```{python}
# Download and load the Excel file
response = rq.get(url3)
response.raise_for_status()  # Raise an exception for HTTP errors

# Read the Excel file directly from the response content
data_local_rent = pd.read_excel(BytesIO(response.content), skiprows=7, engine="openpyxl")
```

```{python}
# Select required columns and remove rows with missing 'price'
listings_filtered = listings_2024[['id', 'room_type', 'price', 'neighbourhood_cleansed']].dropna(subset=['price'])

# Clean and convert 'price' column to numeric
listings_filtered['price'] = listings_filtered['price'].str.replace('[$,]', '', regex=True).astype(float)

# Calculate the average price for all listings
average_airbnb_price = listings_filtered['price'].mean()
print(f"Average Airbnb price per night in 2024: ${average_airbnb_price:.2f}")

# Filter for 'Entire home/apt' room type and calculate its average price
average_entire_home_price = listings_filtered.loc[listings_filtered['room_type'] == 'Entire home/apt', 'price'].mean()
print(f"Average price for Entire home/apt: ${average_entire_home_price:.2f}")

# Calculate the average price by neighbourhood
average_price_by_neighbourhood = listings_filtered.groupby('neighbourhood_cleansed', as_index=False)['price'].mean()
```

```{python}
# Convert 'Time period' column to datetime format
data_local_rent['Time period'] = pd.to_datetime(data_local_rent['Time period'], errors='coerce')

# Filter data for 'London' and year 2024, creating a copy to avoid SettingWithCopyWarning
london_2024_data = data_local_rent[
    (data_local_rent['Region or country name'] == 'London') & 
    (data_local_rent['Time period'].dt.year == 2024)
].copy()

# Calculate daily rental price (assuming 30 days per month)
london_2024_data['Daily rental price (Â£)'] = london_2024_data['Rental price (Â£)'] / 30

# Calculate the average daily rent from January to October 2024
average_daily_rent_2024_10 = london_2024_data['Daily rental price (Â£)'].sum() / 305

# Display the result
print(f"Average daily rental price in London (Jan-Oct 2024): Â£{average_daily_rent_2024_10:.2f}")
```

```{python}
# Calculate annual revenue
voa_revenue_per_annum = average_daily_rent_2024_10 * 365
stl_revenue_per_annum_90 = average_entire_home_price * 90
stl_revenue_per_annum_102 = average_entire_home_price * 102

# Define table data
table_data = {
    'Price Benchmark': ['VOA (mean)', 'STL (mean) 90 nights', 'STL (mean) 102 nights'],
    'Rental Period (nights/year)': [365, 90, 102],
    'Revenue per Night (Â£)': [average_daily_rent_2024_10, average_entire_home_price, average_entire_home_price],
    'Revenue per Annum (Â£)': [voa_revenue_per_annum, stl_revenue_per_annum_90, stl_revenue_per_annum_102]
}

# Create DataFrame
comparison_df = pd.DataFrame(table_data)

# Format DataFrame
styled_df = comparison_df.style.format({
    'Revenue per Night (Â£)': '{:.0f}',
    'Revenue per Annum (Â£)': '{:.0f}'
}).set_table_styles([
    {'selector': 'th', 'props': [('background-color', '#D9EAF7'), 
                                 ('color', 'black'), 
                                 ('text-align', 'center'), 
                                 ('font-weight', 'bold')]},
    {'selector': 'td', 'props': [('text-align', 'center')]},
    {'selector': 'table', 'props': [('border-collapse', 'collapse')]},
    {'selector': 'td, th', 'props': [('border', '1px solid black')]},
    {'selector': 'caption', 'props': [('font-weight', 'bold'), ('text-align', 'center'), ('font-size', '16px')]}
]).set_caption('Table.2 Comparison of Average Nightly Revenue from Airbnb vs Open Market Rent in London')

# Display styled DataFrame
styled_df
```


In Table 2, we reference the Valuation Office Agency's published data to calculate the average daily income from long-term rentals in London (VOA), which amounts to Â£67 per day. Using data scraped from the Inside Airbnb dataset we estimate the average daily income from short-term lets (STL) to be Â£239. 
The estimated annual income for host in the VOA type is approximately Â£24,446. If the STL rental period stays within the legally permitted 90 nights per year, the annual income (Â£21,489) is slightly lower than that of the VOA type. When considering additional costs associated with STL, such as cleaning and vacancy risks, host are unlikely to withdraw their properties from the long-term rental market. However, if host exceed the 90-night limit (estimated at around 102 nights), the economic return from short-term rentals (Â£24,354) surpasses that of long-term rentals in the open market, indicating a high potential for speculative behavior.

```{python}
# Counting the number of Entire home/apt airbnb in London neighborhoods in September 2024
airbnb_24_counts = filtered_listings_2024.groupby('neighbourhood_cleansed').size().reset_index(name='count_24')

filtered_listings_2024 = filtered_listings_2024.merge(
    airbnb_24_counts, on='neighbourhood_cleansed', how='left'
)
```

```{python}
# Filter records where host_total_listings_count > 1 and count unique host_id by neighbourhood_cleansed
PROhost_counts_2024 = (
    filtered_listings_2024[filtered_listings_2024['host_total_listings_count'] > 1]
    .drop_duplicates(subset=['host_id'])
    .groupby('neighbourhood_cleansed')['host_id']
    .nunique()
    .reset_index()
    .rename(columns={'host_id': 'PROhost_counts_2024'})
)

# Filter records where host_total_listings_count == 1 and count unique host_id by neighbourhood_cleansed
single_host_counts_2024 = (
    filtered_listings_2024[filtered_listings_2024['host_total_listings_count'] == 1]
    .drop_duplicates(subset=['host_id'])
    .groupby('neighbourhood_cleansed')['host_id']
    .nunique()
    .reset_index()
    .rename(columns={'host_id': 'single_host_counts_2024'})
)
```

```{python}
# Merge both results into filtered_listings_2024
filtered_listings_2024 = filtered_listings_2024.merge(
    PROhost_counts_2024, on='neighbourhood_cleansed', how='left'
).merge(
    single_host_counts_2024, on='neighbourhood_cleansed', how='left'
)

# Select and rename columns
filtered_listings_2024 = filtered_listings_2024[
    ['neighbourhood_cleansed', 'count_24', 'PROhost_counts_2024', 'single_host_counts_2024']
]
```

```{python}
# Ensure 'Time period' column is treated as a string
data_local_rent['Time period'] = data_local_rent['Time period'].astype(str)

# Filter rows where 'Time period' is between '2023-09' and '2024-09'
filtered_data_23_24 = data_local_rent[
    (data_local_rent['Time period'] >= "2023-09-01") & 
    (data_local_rent['Time period'] <= "2024-09-30") &
    (data_local_rent['Region or country name'] == "London")
]

# Group by 'Area name' and calculate the mean of 'Rental price (Â£)'
average_rent_23_24 = filtered_data_23_24.groupby('Area name', as_index=False)['Rental price (Â£)'].mean()

# Sort the data by 'Rental price (Â£)' in descending order
average_rent_24 = average_rent_23_24.sort_values(by='Rental price (Â£)', ascending=False)
```

```{python}
# Standardize column names for merging
filtered_listings_2024['neighbourhood_cleansed'] = filtered_listings_2024['neighbourhood_cleansed'].str.strip().str.lower()
average_rent_24['Area name'] = average_rent_24['Area name'].str.strip().str.lower()

# Merge datasets and drop redundant column
merged_local_airbnb = filtered_listings_2024.merge(
    average_rent_24,
    left_on='neighbourhood_cleansed',
    right_on='Area name',
    how='inner'
).drop(columns=['Area name'])

# Drop duplicate records by 'neighbourhood_cleansed' and rename 'Rental price (Â£)' column
merged_local_airbnb = (
    merged_local_airbnb.drop_duplicates(subset=['neighbourhood_cleansed'])
    .rename(columns={'Rental price (Â£)': 'rental_price'})
)
```

```{python}
# Prepare area data and merge with merged_local_airbnb
area_data = neighborhood_gdf[['neighbourhood', 'area_km2']].copy()
area_data['neighbourhood'] = area_data['neighbourhood'].str.strip().str.lower()

merged_local_airbnb['neighbourhood_cleansed'] = merged_local_airbnb['neighbourhood_cleansed'].str.strip().str.lower()

# Merge and drop redundant columns
merged_local_airbnb = merged_local_airbnb.merge(
    area_data,
    left_on='neighbourhood_cleansed',
    right_on='neighbourhood',
    how='inner'
).drop(columns=['neighbourhood'])

# Calculate density of count_24 per square kilometer
merged_local_airbnb['count_density'] = merged_local_airbnb['count_24'] / merged_local_airbnb['area_km2']
```

```{python}
# Perform Shapiro-Wilk test for normality on specified variables
variables_1 = ['count_density', 'PROhost_counts_2024', 'rental_price', 'single_host_counts_2024']

for var in variables_1:
    data = merged_local_airbnb[var].dropna()
    stat, p_value = stats.shapiro(data)
    result = "Normal" if p_value > 0.05 else "Not Normal"
    print(f"{var}: Stat={stat:.4f}, p={p_value:.4f}, Result={result}")
```

```{python}
# Apply Box-Cox transformation to specified variables
for var in variables_1:
    # Filter positive values (required for Box-Cox) and drop NaN
    data = merged_local_airbnb[var].dropna().loc[lambda x: x > 0]

    # Perform Box-Cox transformation
    transformed_data, lambda_val = stats.boxcox(data)
    
    # Assign transformed data back to the DataFrame
    merged_local_airbnb.loc[data.index, f'{var}_boxcox'] = transformed_data
```

```{python}
# Perform Shapiro-Wilk test for normality on specified variables
variables_2 = ['count_density_boxcox', 'PROhost_counts_2024_boxcox', 'rental_price_boxcox', 'single_host_counts_2024_boxcox']

for var in variables_2:
    data = merged_local_airbnb[var].dropna()
    stat, p_value = stats.shapiro(data)
    result = "Normal" if p_value > 0.05 else "Not Normal"
    print(f"{var}: Stat={stat:.4f}, p={p_value:.4f}, Result={result}")
```

```{python}
# Set independent variables and dependent variable
x_vars = ['count_density_boxcox', 'PROhost_counts_2024_boxcox', 'single_host_counts_2024_boxcox']
y_var = 'rental_price_boxcox'

# Create scatter plots with regression lines
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
for ax, x_var in zip(axes, x_vars):
    sns.regplot(
        x=merged_local_airbnb[x_var], 
        y=merged_local_airbnb[y_var], 
        ax=ax, 
        scatter_kws={'alpha': 0.6}, 
        line_kws={'color': 'red'}
    )
    ax.set_title(f'{y_var} vs {x_var}')
    ax.set_xlabel(x_var)
    ax.set_ylabel(y_var)

# Add a large title for the entire figure
fig.suptitle("Figure.2 Normality Analysis", fontsize=16, y=1.02)

plt.tight_layout()
plt.show()
```

```{python}
# Calculate correlations with rental_price_boxcox
variables_2 = ['count_density_boxcox', 'PROhost_counts_2024_boxcox', 'rental_price_boxcox', 'single_host_counts_2024_boxcox']

correlations = merged_local_airbnb[variables_2].corr()['rental_price_boxcox'].drop('rental_price_boxcox')

# Output results
print("Correlations with rental_price_boxcox:")
print(correlations)
```

```{python}
# Calculate VIF for independent variables
X = add_constant(merged_local_airbnb[['count_density_boxcox', 'PROhost_counts_2024_boxcox', 'single_host_counts_2024_boxcox']])
vif1 = pd.DataFrame({
    'Variable': X.columns,
    'VIF': [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
})

# Display results
print(vif1)
```

```{python}
# Calculate VIF for independent variables
X = add_constant(merged_local_airbnb[['count_density_boxcox', 'single_host_counts_2024_boxcox']])
vif2 = pd.DataFrame({
    'Variable': X.columns,
    'VIF': [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
})

# Display results
print(vif2)
```

```{python}
# Perform OLS regression using statsmodels
data = merged_local_airbnb[['count_density_boxcox', 'single_host_counts_2024_boxcox', 'rental_price_boxcox']].dropna()
X = sm.add_constant(data[['count_density_boxcox', 'single_host_counts_2024_boxcox']])
y = data['rental_price_boxcox']

model = sm.OLS(y, X).fit()

# visualization
print(model.summary())
```

```{python}
# Calculate residuals and fitted values
residuals, fitted_values = model.resid, model.fittedvalues

# Perform Durbin-Watson test
print(f"Durbin-Watson statistic: {durbin_watson(residuals):.4f}")

# Perform Breusch-Pagan test for heteroscedasticity
lm_stat, p_value, _, _ = het_breuschpagan(residuals, X)
print(f"Breusch-Pagan Test: LM Statistic = {lm_stat:.4f}, p-value = {p_value:.4f}")
```

```{python}
# Split data, train linear regression model, and evaluate RÂ² score
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
lr = LinearRegression().fit(X_train, y_train)
print("R-squared:", r2_score(y_test, lr.predict(X_test)))
```

```{python}
# Align residuals with merged_local_airbnb
merged_local_airbnb['residuals'] = model.resid.reset_index(drop=True)

# Clean and standardize strings
neighborhood_gdf['neighbourhood'] = neighborhood_gdf['neighbourhood'].str.strip().str.lower()
merged_local_airbnb['neighbourhood_cleansed'] = merged_local_airbnb['neighbourhood_cleansed'].str.strip().str.lower()

# Merge geometry with residuals
geo_data = neighborhood_gdf[['neighbourhood', 'geometry']].merge(
    merged_local_airbnb[['neighbourhood_cleansed', 'residuals']],
    left_on='neighbourhood',
    right_on='neighbourhood_cleansed',
    how='inner'
)
```

```{python}
# Create Queen contiguity weights matrix and standardize
w = Queen.from_dataframe(geo_data, use_index=True)
w.transform = 'R'

# Calculate Moran's I for residuals
moran = Moran(geo_data['residuals'], w)
print(f"Moran's I: {moran.I:.4f}, p-value: {moran.p_sim:.4f}")
# print("Significant spatial autocorrelation." if moran.p_sim < 0.05 else "No significant spatial autocorrelation.")

# Plot residuals map
fig, ax = plt.subplots(figsize=(10, 8))
geo_data.plot(
    column='residuals', 
    cmap='coolwarm', 
    edgecolor='black', 
    legend=True, 
    linewidth=0.5, 
    ax=ax
)
ax.set_title("Figure 3: Local Moran's I Residuals Cluster Map")
ax.axis('off')

plt.tight_layout()
plt.show()
```

## References
